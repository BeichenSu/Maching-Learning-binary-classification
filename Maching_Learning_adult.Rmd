---
title: "Maching Learning Adult"
author: "Beichen Su"
date: "5/23/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.Introduction
The data source is found on UCI Machine Learning data sets, called Adult: http://archive.ics.uci.edu/ml/machine-learning-databases/adult/

Prediction task is to determine whether a person makes over 50K
a year. 

Below gives the attributes:

>50K, <=50K.

age: continuous.

workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.

fnlwgt: continuous.

education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.

education-num: continuous.

marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.

occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.

relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.

race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.

sex: Female, Male.

capital-gain: continuous.

capital-loss: continuous.

hours-per-week: continuous.

native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.

## Task: perform logistic regression, random forest and GBM to the data and make prediction.

# 1.1 Data input
The Data contains a train and test set, but has missing value involved. So I combine the data, give it column names and delete missing values.

Take a look at the income, take <=50k as 0 and >50k for 1.

Factor out the categorical data
```{r}
setwd("~/Documents/STAT418/Stat 418/wk-05-ML/HW3")
library(readr)
data1 = read_csv("adult.csv", col_names = FALSE)
data2 = read_csv("adult_test.csv", col_names = FALSE)
originalData = rbind(data1,data2)
rm(data1,data2)
cols = c("age",
         "workclass", 
         "fnlwgt", 
         "education",
         "education_num", 
         "marital_status",
         "occupation",
         "relationship",
         "race",
         "sex",
         "capital_gain",
         "capital_loss",
         "hours_per_week",
         "native_country",
         "income"
         )
colnames(originalData) = cols
# Detect missing value, there is no missing value marked as NA
# There are some ? in the workclass, occupation, native_country
sapply(originalData, function(x) sum(is.na(x)))
unique(originalData$workclass)
unique(originalData$occupation)
unique(originalData$native_country)
originalData = originalData[-which(originalData$workclass == "?"),] 
originalData = originalData[-which(originalData$occupation == "?"),] 
originalData = originalData[-which(originalData$native_country == "?"),] 
# Take a look at the income, take <=50k as 0 and >50k for 1
unique(originalData$income)
index0 = which(originalData$income == "<=50K"| originalData$income == "<=50K.")
index1 = which(originalData$income == ">50K"| originalData$income == ">50K.")
length(index0)+length(index1) == nrow(originalData)
originalData$income[index0] = 0
originalData$income[index1] = 1
# Check the dimension of the data to fit the requirement of the data
table(originalData$income)
dim(originalData)

# Factor out the categorical data
factor_col = c("workclass", 
         "education",
         "marital_status",
         "occupation",
         "relationship",
         "race",
         "sex",
         "native_country",
         "income"
         )
originalData[factor_col] = lapply(originalData[factor_col], factor)
```

# 2. Machine Learning Models

## 2.1 Logistic regression
In this case I use cv.glmnet to the the cross validation to pick the best lambda for lasso regulation(alpha = 1), so that there's no need to seperate the validation set.

### 2.1.1 Spliting data and sparse matrix

I choose to spilt the data 75% for train and 25% for test. Below I split the data and turn them into sparse matrix for glmnet.
```{r}
library(Matrix)
# Split the data
set.seed(12)
trainSize = floor(0.75 * nrow(originalData))
trainInd = sample(seq_len(nrow(originalData)), size = trainSize)
trainSet = originalData[trainInd,]
testSet = originalData[-trainInd,]

X = Matrix::sparse.model.matrix(income~. -1, data = originalData)
X_train = X[trainInd,]
X_test = X[-trainInd,]
```
### 2.1.2 Cross Validation
Do the cross validation so pick the best lambda under lasso regulization.
```{r}
library(glmnet)
system.time({
  cv = cv.glmnet( X_train, trainSet$income,  lambda = seq(0,1,0.001), family = "binomial", alpha = 1)
})
plot(cv)
```

This turns out we want smallest lambda, so that I pick lambda = 0 for this logistic regression model. Now I build the model.

### 2.1.3 Build the train model
```{r}
model_logit = glmnet(X_train, trainSet$income,  lambda = 0, family = "binomial", alpha = 1, intercept = FALSE)
```

### 2.1.4 Performance of the model
Below I give the confusion table, auc and roc as the metric of goodness.
```{r}
pred = predict(model_logit, newx = X_test, type = "response")
# confusion table
tab = table(ifelse(pred > 0.5, 1,0), testSet$income)
tab
# missClassificationError
missClassificationError = 1 - sum(diag(tab))/sum(tab)
missClassificationError
# roc and auc
library(ROCR)
rocr_pred = prediction(pred, testSet$income)
# auc
auc = performance(rocr_pred, "auc")@y.values[[1]]
auc
# roc curve
roc = performance(rocr_pred,"tpr","fpr")
plot(roc, colorize=TRUE)   
abline(a=0,b=1)
```


This model with lambda = 0 gives a really good prediction, as the miss-classification-error is 0.1526623 giving good accuracy, auc is 0.904 and the roc curve looks nice and pretty, which means it will accurately predict a person's income >50K, that based on the true positive rate and the false positive rate. Then it's possible to choose a cut off value depending on we want to precisely predict 0 or 1.




## 2.2 Random Forest

Because there is no build in function for random forest to do cross validation, it's important to split the data in to train, validation and test set. I split the data as 60% for train, 20% for validation and 20% for test.
### 2.2.1 Set up the data
```{r}
trainSize = floor(0.6*nrow(originalData))
trainInd = sample(1:nrow(originalData),trainSize)
trainSet = originalData[trainInd,]
NtrainSet = originalData[-trainInd,]
vSize = floor(0.5*nrow(NtrainSet))
vInd = sample(1:nrow(NtrainSet),vSize)
vSet = NtrainSet[vInd,]
testSet = NtrainSet[-vInd,]
```
### 2.2.2 Employ the validation set to find the optimal tree number
```{r}
library(randomForest)
treeNum = c(10,15,20,50,100,150,200,300,400,500)
aucV = c()
aucT = c()
for(i in 1 : length(treeNum)) {
  md = randomForest(income ~ ., data = trainSet, ntree = treeNum[i])
  tphat = predict(md, trainSet, type = "prob")[,"1"]
  trocr_obj = prediction(tphat, trainSet$income)
  vphat = predict(md, vSet, type = "prob")[,"1"]
  vrocr_obj = prediction(vphat, vSet$income)
  aucT[i] = performance(trocr_obj, "auc")@y.values[[1]]
  aucV[i] = performance(vrocr_obj, "auc")@y.values[[1]]
}
plot(treeNum, aucT,"l", ylim = c(0.8,1), ylab = "AUC")
points(treeNum,aucV, "l",col = "red")
legend(300,0.85,legend = c("Train Set", "Validation Set"), col = c("black","red"), lty = 1:2, cex = 0.6)
abline(v = 200)
```

As I employed the auc as the metric of the goodness of the model, it's clear to see from the graph that after tree number increasing to 200, the AUC doesn't really change. So I choose number of tree equal to 200 for my model.


### 2.2.3 Employ the validation set to find the number of column used in each split, keep number of tree equal to 200.
```{r}
spnum = c(2,3,5,7,9,13)
aucV = c()
aucT = c()
for(i in 1 : length(spnum)) {
  md = randomForest(income ~ ., data = trainSet, ntree = 200, mtry = spnum[i])
  tphat = predict(md, trainSet, type = "prob")[,"1"]
  trocr_obj = prediction(tphat, trainSet$income)
  vphat = predict(md, vSet, type = "prob")[,"1"]
  vrocr_obj = prediction(vphat, vSet$income)
  aucT[i] = performance(trocr_obj, "auc")@y.values[[1]]
  aucV[i] = performance(vrocr_obj, "auc")@y.values[[1]]
}
plot(spnum, aucT,"l", ylim = c(0.8,1), ylab = "AUC")
points(spnum,aucV, "l",col = "red")
legend(10,0.85,legend = c("Train Set", "Validation Set"), col = c("black","red"), lty = 1:2, cex = 0.6)
abline(v = 3)
```

As I employed the auc as the metric of the goodness of the model, it's clear to see that if I increase the column used in each split, the prediction is getting better for my train set but worse on my validation set. It overfits the model as the number of column used. So I choose mtry = 3 for the column used at each split.


### 2.2.4 Employ the validation set to find optimal depth, keep number of tree equal to 200 and metry = 3.
```{r}
dpnum = c(2,3,5,7,9,13,20)
aucV = c()
aucT = c()
for(i in 1 : length(dpnum)) {
  md = randomForest(income ~ ., data = trainSet, ntree = 200, mtry = 3, depth = dpnum[i])
  tphat = predict(md, trainSet, type = "prob")[,"1"]
  trocr_obj = prediction(tphat, trainSet$income)
  vphat = predict(md, vSet, type = "prob")[,"1"]
  vrocr_obj = prediction(vphat, vSet$income)
  aucT[i] = performance(trocr_obj, "auc")@y.values[[1]]
  aucV[i] = performance(vrocr_obj, "auc")@y.values[[1]]
}
plot(dpnum, aucT,"b", ylim = c(0.85,1), ylab = "AUC")
points(dpnum,aucV, "b",col = "red")
legend(15,0.94,legend = c("Train Set", "Validation Set"), col = c("black","red"), lty = 1:2, cex = 0.6)
abline(v = 5)
```

It looks like there's no serious over-training effects for the depth of the tree, and depth equal to 5 is the optimal choice.

### 2.2.5 With all parameter selected by validation set, fit the model and see the goodness of fit on the test data set. (number of tree = 200, mtry = 3, depth = 5)

```{r}
model_RF = randomForest(income ~ ., data = trainSet, ntree = 200, mtry = 3, depth = 5)
testphat = predict(md, testSet, type = "prob")[,"1"]
test_obj = prediction(testphat, testSet$income)
aucTest = performance(test_obj, "auc")@y.values[[1]]
aucTest
roc = performance(test_obj,"tpr","fpr")
plot(roc, colorize=TRUE)   
abline(a=0,b=1)
```

This model gives auc at 0.8883794, and the ROC curve look pretty nice. It looks like the cutoff point should be really close to 0.1 to gain the high TPR and low FPR.

## 2.3 GBM
### 2.3.1 Data input and subset
As there is a build in function to do the cross validation and model selection feature, I don't split a validation set. As the dependent variable only takes numeric input, I turn the income to numeric.
```{r}
# without h2o
library(gbm)
set.seed(12)
originalData$income = as.numeric(originalData$income)
originalData$income = ifelse(originalData$income > 1, 1,0)
trainSize = floor(0.75 * nrow(originalData))
trainInd = sample(seq_len(nrow(originalData)), size = trainSize)
trainSet = originalData[trainInd,]
testSet = originalData[-trainInd,]
```

### 2.3.2 The optimal number of tree and tuning of learning ratio and depth.
I build a few gbm models based on its build-in validation algorithm. And the task is to find out the relationship between those parameters.

First I have a model with depth = 5 and learning ratio = 0.6
```{r}
md = gbm(trainSet$income ~ ., data = trainSet, distribution = "bernoulli",
          n.trees = 100, interaction.depth = 5, shrinkage = 0.6, cv.folds = 5)
gbm.perf(md, plot.it = TRUE)
```

This model suggests that 37 trees should be sufficient to avoid overfitting.

Now I tune the depth up to 10, and learning ratio as the same.
```{r}
md = gbm(trainSet$income ~ ., data = trainSet, distribution = "bernoulli",
          n.trees = 100, interaction.depth = 10, shrinkage = 0.6, cv.folds = 5)
gbm.perf(md, plot.it = TRUE)
```

It turns out that only 19 trees are needed, which implies that increasing in depths results in less tree.

Below I keep the depth at 5 and tune the learning ratio down to 0.3.
```{r}
md = gbm(trainSet$income ~ ., data = trainSet, distribution = "bernoulli",
          n.trees = 100, interaction.depth = 5, shrinkage = 0.3, cv.folds = 5)
gbm.perf(md, plot.it = TRUE)
```

It turns out that 89 trees are needed, which implies that decreasing in learning ratio results in more tree.









